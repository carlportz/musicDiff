{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_to_npz_files, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.data = []\n",
    "        for npz_file in os.listdir(path_to_npz_files):\n",
    "            data = np.load(os.path.join(path_to_npz_files, npz_file))\n",
    "            data = data['q_nm']\n",
    "            data = np.array([data.real, data.imag])\n",
    "            data = np.transpose(data, (1, 2, 0))\n",
    "            self.data.append(data)\n",
    "        self.data = np.array(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transforms(self.data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    # normalize data by largest norm across all channels\n",
    "    transforms.Lambda(lambda x: x / torch.max(torch.norm(x, dim=0)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_npz_files = './data/training_npz'\n",
    "dataset = CustomDataset(path_to_npz_files, transforms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Data shape torch.Size([16, 2, 128, 128])\n",
      "Batch 1: Data shape torch.Size([16, 2, 128, 128])\n",
      "Batch 2: Data shape torch.Size([16, 2, 128, 128])\n",
      "Batch 3: Data shape torch.Size([16, 2, 128, 128])\n",
      "Batch 4: Data shape torch.Size([16, 2, 128, 128])\n",
      "Batch 5: Data shape torch.Size([16, 2, 128, 128])\n",
      "Batch 6: Data shape torch.Size([16, 2, 128, 128])\n",
      "Batch 7: Data shape torch.Size([16, 2, 128, 128])\n",
      "Batch 8: Data shape torch.Size([16, 2, 128, 128])\n",
      "Batch 9: Data shape torch.Size([16, 2, 128, 128])\n",
      "Batch 10: Data shape torch.Size([16, 2, 128, 128])\n",
      "Batch 11: Data shape torch.Size([9, 2, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(dataloader):\n",
    "    # Your training code here\n",
    "    print(f\"Batch {batch_idx}: Data shape {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.8602e-03,  7.8000e-03, -1.6978e-02,  ...,  5.8471e-03,\n",
      "          -5.3269e-03,  3.2776e-03],\n",
      "         [-7.8214e-03,  2.8394e-02, -3.3722e-02,  ...,  4.3092e-03,\n",
      "          -1.8106e-02,  2.0748e-03],\n",
      "         [-5.3661e-02,  8.2054e-02, -4.0894e-02,  ...,  2.5697e-02,\n",
      "           4.3883e-02, -4.4351e-02],\n",
      "         ...,\n",
      "         [ 1.6702e-04, -2.7817e-03,  2.4416e-03,  ...,  1.4293e-03,\n",
      "           5.4289e-04, -1.1706e-03],\n",
      "         [ 4.5386e-03, -4.5168e-03,  1.0166e-03,  ...,  1.8771e-03,\n",
      "          -1.9520e-03,  2.4651e-03],\n",
      "         [ 2.0920e-03,  1.5561e-03, -1.4456e-02,  ...,  1.3432e-02,\n",
      "           2.9100e-03, -1.3333e-02]],\n",
      "\n",
      "        [[ 4.1448e-03, -9.0129e-03,  1.2712e-02,  ..., -2.5560e-04,\n",
      "           8.9126e-03, -9.2853e-03],\n",
      "         [ 3.3426e-03, -1.3396e-02,  3.8496e-02,  ...,  1.0731e-03,\n",
      "           1.6575e-02, -2.3991e-02],\n",
      "         [ 3.2256e-05, -6.8214e-02,  7.1803e-02,  ...,  5.1753e-02,\n",
      "           2.6384e-02, -7.8400e-03],\n",
      "         ...,\n",
      "         [ 1.5355e-03, -1.1309e-04, -1.6065e-03,  ..., -7.1328e-03,\n",
      "          -9.8548e-04, -3.2899e-04],\n",
      "         [ 3.8604e-03,  8.9086e-04,  6.3609e-04,  ..., -1.0747e-02,\n",
      "           3.7958e-03, -1.2091e-03],\n",
      "         [ 2.0656e-02,  7.2528e-03, -2.2842e-02,  ..., -3.6345e-02,\n",
      "          -5.9490e-03,  5.4915e-03]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
